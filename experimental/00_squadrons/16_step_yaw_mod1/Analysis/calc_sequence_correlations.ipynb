{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'svg' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import group_data as gd\n",
    "import flylib\n",
    "import pylab as plb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swarm_data = dict()\n",
    "for key in gd.swarms.keys():\n",
    "    paths = dict()\n",
    "    for fly in gd.swarms[key].flies:\n",
    "        paths[fly.fly_num] =  {'fly_path':fly.fly_path,'fly_num':fly.fly_num}\n",
    "    swarm_data[key] = paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_line_database(line_name):\n",
    "    line_name = line_name.split('_GFP')[0].split('GMR')[1]\n",
    "    import cPickle\n",
    "    f = open('../../../mn_expression_matrix_plot/line_database.cpkl','rb')\n",
    "    line_database = cPickle.load(f)\n",
    "    f.close()\n",
    "    return line_database\n",
    "\n",
    "def get_muscle_list(line_name):\n",
    "    line_database = get_line_database(line_name)\n",
    "    ln = line_name.split('_GFP')[0].split('GMR')[1]\n",
    "    muscle_names = list()\n",
    "    for key in line_database[ln].keys():\n",
    "        if line_database[ln][key] > 0:\n",
    "            muscle_names.append(key)\n",
    "    muscle_names = sorted(muscle_names)\n",
    "    #muscle_names = sorted(get_muscle_list(line_name))\n",
    "    return muscle_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_signal_dict(fly_path):\n",
    "    import h5py\n",
    "    import cPickle\n",
    "    import numpy as np\n",
    "    muscle_data_file = fly_path+'nnls_fits_no_bk_dF_F.cpkl'\n",
    "    fly_record_file = fly_path+'fly_record.hdf5'\n",
    "    signal_dict = dict()\n",
    "    fi = open(muscle_data_file,'rb')\n",
    "    muscle_data = cPickle.load(fi)\n",
    "    fi.close()\n",
    "    for mu,sig in zip(muscle_data['muscles'],muscle_data['fits']):\n",
    "        signal_dict[mu] = sig\n",
    "    hf = h5py.File(fly_record_file,'r')\n",
    "    exp_record = hf['experiments'].values()[0]\n",
    "    for key in exp_record['tiff_data']['axon_framebase'].keys():\n",
    "        signal_dict[key] = np.array(exp_record['tiff_data']['axon_framebase'][key])\n",
    "    hf.close()\n",
    "    return signal_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "[Errno 2] No such file or directory: '/media/FlyDataC/FlyDB/Fly0313/nnls_fits_no_bk_dF_F.cpkl'\n"
     ]
    }
   ],
   "source": [
    "for swarm in swarm_data.values():\n",
    "    for flykey in swarm.keys():\n",
    "        fly = swarm[flykey]\n",
    "        try:\n",
    "            #print fly['fly_num']\n",
    "            fly['sigs'] = get_signal_dict(fly['fly_path'])\n",
    "        except IOError as er:\n",
    "            print flykey\n",
    "            swarm.pop(flykey)\n",
    "            print er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_muscle_list(line_name):\n",
    "    \"\"\"function to get list of muscles for a given line \n",
    "    should make this general and move to src/ \"\"\"\n",
    "    line_database = get_line_database(line_name)\n",
    "    ln = line_name.split('_GFP')[0].split('GMR')[1]\n",
    "    muscle_names = list()\n",
    "    for key in line_database[ln].keys():\n",
    "        if line_database[ln][key] > 0:\n",
    "            muscle_names.append(key)\n",
    "    muscle_names = sorted(muscle_names)\n",
    "    #muscle_names = sorted(get_muscle_list(line_name))\n",
    "    return muscle_names\n",
    "\n",
    "def map_cross_cors(fly_num,sigs,muscle_list):\n",
    "    \"\"\"parallel function to calculate cross correlations\"\"\" \n",
    "    def calc_cross_cors(sigs,muscle_list):\n",
    "        import flylib\n",
    "        import numpy as np\n",
    "        #reload(flylib)\n",
    "        amp_corr_dict = dict()\n",
    "        freq_corr_dict = dict()\n",
    "        auto_corr_dict = dict()\n",
    "        import flylib\n",
    "        rwing = flylib.butter_bandpass_filter(sigs['Ph1'],0.02,20.0,0.022,order = 1)\n",
    "        wbf = sigs['wb_frequency']\n",
    "        wbf_filt = flylib.butter_bandpass_filter(wbf,0.02,20.0,0.022,order = 1)\n",
    "        StimCond = sigs['StimCond']\n",
    "        imlen = np.shape(sigs[muscle_list[0]])[0]\n",
    "        mask = ((StimCond<0) & (wbf >150))[:imlen]\n",
    "        times = sigs['times']\n",
    "        dt = (times[1]-times[0])\n",
    "        xcor_times = np.arange(0,np.sum(mask))*dt - dt*np.sum(mask)/2\n",
    "        from scipy import signal\n",
    "        for mname in muscle_list:\n",
    "            if not('ps' in mname):\n",
    "                msig = flylib.butter_bandpass_filter(sigs[mname],0.02,20.0,0.022,order = 1)\n",
    "                amp_corr_dict[mname] = signal.correlate(msig[mask],rwing[mask],mode = 'same')\n",
    "                freq_corr_dict[mname] = signal.correlate(msig[mask],wbf_filt[mask],mode = 'same')\n",
    "                auto_corr_dict[mname] = signal.correlate(msig[mask],msig[mask],mode = 'same')\n",
    "        return {'amp':amp_corr_dict,'freq':freq_corr_dict,'auto':auto_corr_dict,'times':xcor_times}\n",
    "    return {fly_num:calc_cross_cors(sigs,muscle_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a tuple of arguments to pass to the map function\n",
    "map_data = dict()\n",
    "for swarmkey in swarm_data.keys():\n",
    "    muscle_list = get_muscle_list(swarmkey)\n",
    "    map_data[swarmkey] = ([key for key in swarm_data[swarmkey].keys()],\n",
    "                           [swarm_data[swarmkey][key]['sigs'] for key in swarm_data[swarmkey].keys()],\n",
    "                           [muscle_list for key in swarm_data[swarmkey].keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "#get the ipython clients\n",
    "from IPython.parallel import Client\n",
    "clients = Client() \n",
    "clients.block = True\n",
    "print clients.ids\n",
    "v = clients[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMR22H05_GFP\n"
     ]
    }
   ],
   "source": [
    "#map in chunks by swarm\n",
    "output_dict = dict()\n",
    "for swarmkey in swarm_data.keys():\n",
    "    print swarmkey\n",
    "    output_dict[swarmkey] = v.map(map_cross_cors,*map_data[swarmkey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the computed data back into the swarm_data dictionary\n",
    "for swarmkey in swarm_data.keys():\n",
    "    for i,fnum in enumerate([x.keys()[0] for x in output_dict[swarmkey]]):\n",
    "        swarm_data[swarmkey][fnum]['corr_data'] =  output_dict[swarmkey][i].values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the dictionary as a pickle (need to write that dict to hdf5 wrapper)\n",
    "import cPickle\n",
    "f = open('extracted_sigs.cpkl','wb')\n",
    "for swarmkey in swarm_data.keys():\n",
    "    for fly in swarm_data[swarmkey].values():\n",
    "        print fly['fly_path']\n",
    "        print fly.keys()\n",
    "        data_path =  fly['fly_path'] + 'nnls_fits_no_bk_dF_F_corr_data.cpkl'\n",
    "        f = open(data_path,'wb')\n",
    "        cPickle.dump(fly['corr_data'],f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the computed data back into the swarm_data dictionary\n",
    "for swarmkey in swarm_data.keys():\n",
    "    for i,fnum in enumerate([x.keys()[0] for x in output_dict[swarmkey]]):\n",
    "        swarm_data[swarmkey][fnum]['corr_data'] =  output_dict[swarmkey][i].values()[0]\n",
    "\n",
    "#save the dictionary as a pickle (need to write that dict to hdf5 wrapper)\n",
    "import cPickle\n",
    "f = open('extracted_sigs.cpkl','wb')\n",
    "for swarmkey in swarm_data.keys():\n",
    "    for fly in swarm_data[swarmkey].values():\n",
    "        print fly['fly_path']\n",
    "        print fly.keys()\n",
    "        data_path =  fly['fly_path'] + 'nnls_fits_no_bk_dF_F_sig_mtrx.cpkl'\n",
    "        f = open(data_path,'wb')\n",
    "        cPickle.dump(fly['sig_mtrx'],f)\n",
    "        f.close()\n",
    "        \n",
    "        data_path =  fly['fly_path'] + 'nnls_fits_no_bk_dF_F_corr_data.cpkl'\n",
    "        f = open(data_path,'wb')\n",
    "        cPickle.dump(fly['corr_data'],f)\n",
    "        f.close()\n",
    "        \n",
    "        data_path =  fly['fly_path'] + 'nnls_fits_no_bk_dF_F_sigs.cpkl'\n",
    "        f = open(data_path,'wb')\n",
    "        cPickle.dump(fly['sigs'],f)\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
